{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "# Importing libraries\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "from collections import deque\n",
    "import collections\n",
    "import pickle\n",
    "\n",
    "#for text processing\n",
    "import spacy\n",
    "import re\n",
    "import pandas as pd\n",
    "env = gym.make(\"Taxi-v2\").env\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### There are 4 locations (labeled by different letters), and our job is to pick up the passenger at one location and drop him off at another. We receive +20 points for a successful drop-off and lose 1 point for every time-step it takes. There is also a 10 point penalty for illegal pick-up and drop-off actions.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetching Origing, Destination, and Time of Pickup from the sms data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_pickup_drop(sms):\n",
    "    \n",
    "    #Write your code here\n",
    "    location_df = pd.read_csv('city.csv')\n",
    "    cities = list(location_df['location'])\n",
    "    sms_cities = []\n",
    "    origin =[]\n",
    "    dest = []\n",
    "    time_of_pickup = []\n",
    "    \n",
    "    for city in cities:\n",
    "        if city in sms:\n",
    "            sms_cities.append(city)\n",
    "            \n",
    "    from_match = re.findall(r\"from\\s\\w+\\s\\w+\\s\\w+\" , sms)\n",
    "    from_match = list(from_match)\n",
    "    \n",
    "    to_match = re.findall(r\"to\\s\\w+\\s\\w+\\s\\w+\" , sms)\n",
    "    to_match = list(to_match)\n",
    "    \n",
    "    for city in sms_cities:\n",
    "        if (len(from_match)>0):\n",
    "            if(city in from_match[0]):\n",
    "                origin.append(city)\n",
    "                sms_cities.remove(city)\n",
    "                dest.append(sms_cities[0])\n",
    "        else:\n",
    "            if (len(to_match)>0):\n",
    "                if(city in to_match[0]):\n",
    "                    dest.append(city)\n",
    "                    sms_cities.remove(city)\n",
    "                    origin.append(sms_cities[0])\n",
    "    time = re.findall(r\"(\\d+ PM)|(\\d+ AM)\",sms)\n",
    "    if time[0][0] == '':\n",
    "        time_of_pickup = time[0][1]\n",
    "    else:\n",
    "        time_of_pickup = time[0][0]\n",
    "    \n",
    "    return [origin, dest, time_of_pickup]\n",
    "    \n",
    "#f = open(\"sms.txt\", \"r\")\n",
    "#num_of_lines = 1000\n",
    "#for line in f:\n",
    "#    fetch_pickup_drop(line)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :\u001b[43mG\u001b[0m|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "\n",
      "Action Space Discrete(6)\n",
      "State Space Discrete(500)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<gym.envs.toy_text.taxi.TaxiEnv at 0x7f2a26732940>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset() # reset environment to a new, random state\n",
    "env.render()\n",
    "\n",
    "print(\"Action Space {}\".format(env.action_space))\n",
    "print(\"State Space {}\".format(env.observation_space))\n",
    "env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summing up the Q-Learning Process\n",
    "\n",
    "\n",
    "Initialize the Q-table by all zeros.\n",
    "\n",
    "Start exploring actions: \n",
    "\n",
    "For each state, select any one among all possible actions for the current state (S).\n",
    "\n",
    "Travel to the next state (S') as a result of that action (a).\n",
    "\n",
    "For all possible actions from the state (S') select the one with the highest Q-value.\n",
    "\n",
    "Update Q-table values using the equation.\n",
    "\n",
    "Set the next state as the current state.\n",
    "\n",
    "If goal state is reached, then end and repeat the process.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploiting learned values\n",
    "After enough random exploration of actions, the Q-values tend to converge serving our agent as an action-value function which it can exploit to pick the most optimal action from a given state.\n",
    "\n",
    "There's a tradeoff between exploration (choosing a random action) and exploitation (choosing actions based on already learned Q-values). We want to prevent the action from always taking the same route, and possibly overfitting, so we'll be introducing another parameter called Ïµ \"epsilon\" to cater to this during training.\n",
    "\n",
    "Instead of just selecting the best learned Q-value action, we'll sometimes favor exploring the action space further. Lower epsilon value results in episodes with more penalties (on average) which is obvious because we are exploring and making random decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize Q_table\n",
    "import numpy as np\n",
    "#write your code here\n",
    "\n",
    "q_table = np.zeros([env.observation_space.n, env.action_space.n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 100000\n",
      "CPU times: user 44.4 s, sys: 6.45 s, total: 50.8 s\n",
      "Wall time: 9min 15s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\"\"\"Training the agent\"\"\"\n",
    "\n",
    "import random\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Hyperparameters\n",
    "alpha = 0.1\n",
    "gamma = 0.6\n",
    "epsilon = 0.1\n",
    "\n",
    "# For plotting metrics\n",
    "all_epochs = []\n",
    "all_penalties = []\n",
    "\n",
    "##Write your code here\n",
    "for i in range(1, 100001):\n",
    "    state = env.reset()\n",
    "\n",
    "    epochs, penalties, reward, = 0, 0, 0\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        if random.uniform(0, 1) < epsilon:\n",
    "            action = env.action_space.sample() # Explore action space\n",
    "        else:\n",
    "            action = np.argmax(q_table[state]) # Exploit learned values\n",
    "\n",
    "        next_state, reward, done, info = env.step(action) \n",
    "        \n",
    "        old_value = q_table[state, action]\n",
    "        next_max = np.max(q_table[next_state])\n",
    "        \n",
    "        new_value = (1 - alpha) * old_value + alpha * (reward + gamma * next_max)\n",
    "        q_table[state, action] = new_value\n",
    "\n",
    "        if reward == -10:\n",
    "            penalties += 1\n",
    "\n",
    "        state = next_state\n",
    "        epochs += 1\n",
    "        \n",
    "    if i % 100 == 0:\n",
    "        clear_output(wait=True)\n",
    "        print(f\"Episode: {i}\")\n",
    "np.save(\"./q_table.npy\", q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0.        ,   0.        ,   0.        ,   0.        ,\n",
       "          0.        ,   0.        ],\n",
       "       [ -2.2732518 ,  -2.12208638,  -2.27325181,  -2.1220864 ,\n",
       "         -1.870144  , -11.12208572],\n",
       "       [ -1.870144  ,  -1.45024002,  -1.87014399,  -1.4502401 ,\n",
       "         -0.7504    , -10.45023826],\n",
       "       ...,\n",
       "       [ -0.93352002,   0.416     ,  -1.05223237,  -1.19427342,\n",
       "         -6.05058976,  -4.64766784],\n",
       "       [ -2.146069  ,  -2.1220551 ,  -2.17056679,  -2.12205477,\n",
       "         -3.69883525,  -3.72068937],\n",
       "       [  3.10561091,   1.76835778,   2.75819295,  11.        ,\n",
       "         -2.25208763,  -2.6470922 ]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load trained q_table for evaluation\n",
    "q_table = np.load(\"./q_table.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_loc_dict(city_df):\n",
    "    loc_dict = {}\n",
    "    ## Create dictionary example, loc_dict['dwarka sector 23] = 0\n",
    "    for index ,row in city_df.iterrows():\n",
    "        loc_dict[row['location']] = row['mapping']     \n",
    "    return loc_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_pick_up_drop_correction(pick_up, drop, line_num):\n",
    "    #write your code here\n",
    "    orig_df = pd.read_csv('org_df.csv')\n",
    "    original_origin = orig_df.iloc[line_num]['origin']\n",
    "    original_destination = orig_df.iloc[line_num]['dest']\n",
    "    if original_origin == pick_up and original_destination == drop:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results after 1000 episodes:\n",
      "Average timesteps per episode: 13.43\n",
      "Average penalties per episode: 1.03\n",
      "Total number of wrong predictions  1000\n",
      "Total Reward is  10000\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Evaluate agent's performance after Q-learning\"\"\"\n",
    "\n",
    "# 1) We need to take text drom \"sms.txt\" and fetch pickup and drop from it.\n",
    "# 2) Generate the random state from an enviroment and change the pick up and drop as the fetched one\n",
    "# 3) Evaluate you q_table performance on all the texts given in sms.txt.\n",
    "# 4) Have a check if the fetched pickup, drop is not matching with original pickup, drop using orig.csv\n",
    "# 5) If fetched pickup or/and drop does not match with the original, add penality and reward -10\n",
    "# 6) Calculate the Total reward, penalities, Wrong pickup/drop predicted and Average time steps per episode.\n",
    "\n",
    "total_epochs, total_penalties, total_reward, wrong_predictions = 0, 0, 0, 0\n",
    "\n",
    "info = pd.DataFrame(columns=['origin','destination','time'])\n",
    "count = 0\n",
    "time_list = []\n",
    "f = open(\"./sms.txt\", \"r\")\n",
    "num_of_lines = 1000\n",
    "city = pd.read_csv(\"./city.csv\")\n",
    "\n",
    "loc_dict = create_loc_dict(city)\n",
    "line_num = 0\n",
    "for line in f:\n",
    "    l = fetch_pickup_drop(line)\n",
    "    pick_up = l[0]\n",
    "    drop = l[1]\n",
    "    decision = check_pick_up_drop_correction(pick_up,drop,line_num)\n",
    "    if not decision:\n",
    "        total_penalties += 1\n",
    "        reward = -10\n",
    "        total_reward += reward\n",
    "        wrong_predictions += 1\n",
    "    pickUP_idx = loc_dict[pick_up[0]]\n",
    "    drop_idx = loc_dict[drop[0]]\n",
    "    act_state = env.reset()\n",
    "    taxi_row, taxi_col,pick_up ,drop  = env.decode(act_state)\n",
    "    state = env.encode(taxi_row,taxi_col,int(pickUP_idx),int(drop_idx))\n",
    "    epochs, penalties, reward = 0, 0, 0\n",
    "    \n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        action = np.argmax(q_table[state])\n",
    "        state, reward, done, info = env.step(action)\n",
    "\n",
    "        if reward == -10:\n",
    "            penalties += 1\n",
    "\n",
    "        epochs += 1\n",
    "\n",
    "    total_penalties += penalties\n",
    "    total_epochs += epochs\n",
    "    total_reward += reward\n",
    "\n",
    "\n",
    "\n",
    "print(f\"Results after {num_of_lines} episodes:\")\n",
    "print(f\"Average timesteps per episode: {total_epochs / num_of_lines}\")\n",
    "print(f\"Average penalties per episode: {total_penalties / num_of_lines}\")\n",
    "print(f\"Total number of wrong predictions \", wrong_predictions)\n",
    "print(\"Total Reward is \", total_reward)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
